# Getting started with high-throughput data processing on the Spider platform

**Content**

Vastly increasing data volumes and data complexity imply that researchers require novel solutions to large scale cluster
computing. This workshop aims to provide an introduction and hands-on experience with our new solution to this problem: 
"a new and versatile platform for high-throughput data processing". The objective of the workshop is to distinguish the 
different functionalities of the platform, to provide information on applying for access, and to give participants 
hands-on experience with basic job processing, software distribution and portability, using internal and external 
storage systems, and how to collaborate on data analysis within a project. Participants will acquire first hand experience
with the flexibility, interactivity and interoperability offered by the  platform.

**Target group / Prerequisites**

Anyone who wants to start processing large data volumes (tens to hundreds of terabytes or even more)

  - Familiarity with Linux commands
  
  - Familiarity with batch systems (ssh access, job submission), preferably you have followed Introduction to cluster computing course (cluster-computing)

**Where**: 3.5 in SURF Utrecht

**When**: 13:00-17:00 28 August 2019 (Wednesday)

**Registration**: https://sara-nl.github.io/2019-08-28-htdp-elixir/

**Program**:
--------
13:00 - 13:20 - Introduction to the high throughput platform

13:20 - 13:45 - [Login to the platform](login-to-spider.md)

13:45 - 14:45 - [Spider project spaces and roles](demo-spider-roles.md)

14:45 - 15:00 - Coffee

15:00 - 16:00 - [Running data analysis](run-spider-jobs.md)

16:15 - 16:45 - [Additional features on the platform](extras/README.md)

16:45 - 17:00 - [Conclusion]

