## Performance of staging and scratch area

### Background information

The internal project that delivers the Ceph storage to Spider is Apollo and it uses HDD (spinning) disk technology to provide Ceph as a storage solution. The read/write speeds of modern HDD disks are typically around 150 Mbyte/sec. Ceph utilizes an array of disks to increase the write/read speeds of data via CephFS to obtain a performance that is higher than that of single disk. This works particularly well for writing data as his is mainly governed by CephFS. It works less well for reading as this is mainly governed by your application. Also note that on Ceph your data has been setup with a 3-fold redundancy to prevent data loss as consequency of disk failures.
The disk storage on the Spider WNs is either HDD or SSD and can be selected via the Slurm --constraint option. Here we focus on the local storage in terms of the SSDs only as an example to clarify the difference in speeds obtained when running your data processing on a shared resource vs. a local resource. The SSDs in Spider are of the NVMe type. These NVMe SSD disks provide write speeds up to 3500 Mbyte/sec, whereas standard SATA SSD disks provide write/read speeds of 500/530 Mbyte/sec. The local resource mode for data storage (and processing) for some of you may be reminiscent of the way in which the high-throughput Grid platform is utilized and this is intended as such.
